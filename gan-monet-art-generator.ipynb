{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Visualizations\nimport seaborn as sns # More visualizations\nimport os # Deal with the file system\nfrom tqdm.notebook import tqdm, trange # loader bar\nimport urllib.parse\nfrom kaggle_datasets import KaggleDatasets # The images for this lab come from KaggleDataSets\nimport PIL # For saving images\nimport shutil # For zipping images to output file\n\nimport tensorflow as tf\nimport keras\n\n# TFRecord image extraction\nfrom tensorflow import string, cast, reshape, float32\nfrom tensorflow.io.gfile import glob\nfrom tensorflow.io import FixedLenFeature, parse_single_example\nfrom tensorflow.image import decode_jpeg\nfrom tensorflow.data import TFRecordDataset, Dataset\n\n# Model building\nfrom tensorflow import random_normal_initializer, GradientTape, ones_like, zeros_like, reduce_mean, abs\nfrom keras.initializers import RandomNormal\nfrom keras import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Input, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Concatenate, ZeroPadding2D\nfrom keras.losses import BinaryCrossentropy\nfrom tensorflow.compat.v1.losses import Reduction\nfrom keras.optimizers import Adam\nfrom tensorflow_addons.layers import InstanceNormalization\n\nINPUT_DIR = \"/kaggle/input/gan-monet\"\nWORKING_DIR = \"/kaggle/working\"\nRANDOM_STATE = 1 # Set to None if reproducibility is not needed\nDEBUG_SAMPLE_FRAC = 1 # Reduce the amount of data used so that debugging the notebook is faster\n\n# Set up TPU usage\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-02T21:45:44.267575Z","iopub.execute_input":"2023-05-02T21:45:44.268387Z","iopub.status.idle":"2023-05-02T21:45:55.334285Z","shell.execute_reply.started":"2023-05-02T21:45:44.268330Z","shell.execute_reply":"2023-05-02T21:45:55.333124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Brief Introduction to the Problem and the Data\nThis competition is an exciting foray into the world of AI art generation.  \n\n## The Problem \nGiven a set of images to draw inspiration from, train an AI model to generate images that are as indiscernable from images in that set as can be managed. \n\nIn fact, with the approach of a generative adversarial network, a discriminator network will also be trained to learn which images which belong in the set and will be coupled with the generator.  This way, as the discriminator gets better at discerning if an image belongs in the set, the generator gets better at making images that look like they belong in that same set.\n\nWith the trained generator, generate 7K to 10K Monet style images.\n\n## The Data\nThe data is comprised of two sets of images: Monet paintings and source photos.  Each set is offered in two different image formats: JPEG and TFRecord. The images are 256x256 pixels\n\nThe paintings will be used to train the discriminator, and the photos will be used as inspiration for the generator.\n\nFor this approach, the TFRecord versions of the images will be used.  \n\nThe generated images will be bundled in images.zip and each generated image will be 256x256 pixels.\n\n# Exploratory Data Analysis\nThe image data is loaded from the image source.  The Monet painting images will be loaded separately from the photographs.  \n\nFirst, the image filenames are loaded","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path()\nMONET_FILENAMES = glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:45:55.339564Z","iopub.execute_input":"2023-05-02T21:45:55.342541Z","iopub.status.idle":"2023-05-02T21:45:55.914088Z","shell.execute_reply.started":"2023-05-02T21:45:55.342496Z","shell.execute_reply":"2023-05-02T21:45:55.912884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A routine is defined to load the image data from a TFRecord, and a method that will then use the first routine to extract image data from a list of filenames.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\n\ndef decode_image(image):\n    image = decode_jpeg(image, channels=3)\n    image = (cast(image, float32) / 127.5) - 1 # Center the image pixel value on zero (-1 <= px <= 1)\n    image = reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": FixedLenFeature([], string),\n        \"image\": FixedLenFeature([], string),\n        \"target\": FixedLenFeature([], string)\n    }\n    example = parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    dataset = TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:45:55.915721Z","iopub.execute_input":"2023-05-02T21:45:55.916110Z","iopub.status.idle":"2023-05-02T21:45:55.926411Z","shell.execute_reply.started":"2023-05-02T21:45:55.916073Z","shell.execute_reply":"2023-05-02T21:45:55.924519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, the datasets are loaded into TFRecordDatasets","metadata":{}},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:45:55.929534Z","iopub.execute_input":"2023-05-02T21:45:55.931831Z","iopub.status.idle":"2023-05-02T21:46:00.063937Z","shell.execute_reply.started":"2023-05-02T21:45:55.931793Z","shell.execute_reply":"2023-05-02T21:46:00.062842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An example is selected from each dataset to render and observe.","metadata":{}},{"cell_type":"code","source":"example_monet = next(iter(monet_ds))\nplt.subplot(1, 2, 1)\nplt.title(\"Monet\")\nplt.imshow(example_monet[0]*0.5 + 0.5) # Restore pixel value to 0 - 1\n\nexample_photo = next(iter(photo_ds))\nplt.subplot(1, 2, 2)\nplt.title(\"Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5) # Restore pixel value to 0 - 1","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:00.065395Z","iopub.execute_input":"2023-05-02T21:46:00.065756Z","iopub.status.idle":"2023-05-02T21:46:01.106150Z","shell.execute_reply.started":"2023-05-02T21:46:00.065720Z","shell.execute_reply":"2023-05-02T21:46:01.105040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture\nIn order to generate convincing Monet art, the data model will train as a Generative Adversarial Network, where a generator is trained to trick a discriminator as the discriminator is trained to identify originals from generated output.\n\nSince the input dataset offers source photos that we can translate to a Monet style, a CycleGAN architecture can be used with great effect.  CycleGANs are good for translating images from one domain to another, such as translating photo from photorealistic style to a Monet art style.\n\nAdditionally, the GAN will be designed with a UNET architecture.  A UNET design has a contracting path and an expansive path. The contracting path is responsible for extracting features from the input image, while the expansive path is responsible for upsampling the features and reconstructing the output image.\n\nThe TFRecord image type is selected and routines defined above load image data from the input set.  Loss functions for different types of translations will be built and will allow comparing output of the discriminator and the generator with original, generated, cycled, and same images. \n\nThe model will then be compiled using an Adam optimizer, which is a popular choice to tune hyperparameters during the model fit phase.\n\n## UNET Downsampler and Upsampler\nThe first path of the UNET downsamples the input image using CNNs with a stride of 2, which will reduce the width and height by 2 each pass.  The downsample routine is defined as below.","metadata":{}},{"cell_type":"code","source":"def downsample(num_filters, kernel_size, apply_instancenorm=True):\n    normal_init = random_normal_initializer(0, 0.02)\n    gamma_init = RandomNormal(mean=0, stddev=0.02)\n\n    # Build the model for the downsampler\n    model = Sequential()\n    model.add(Conv2D(num_filters, kernel_size, strides=2, padding='same', kernel_initializer=normal_init, use_bias=False))\n\n    if apply_instancenorm:\n        model.add(InstanceNormalization(gamma_initializer=gamma_init))\n\n    model.add(LeakyReLU())\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:01.107110Z","iopub.execute_input":"2023-05-02T21:46:01.107458Z","iopub.status.idle":"2023-05-02T21:46:01.115428Z","shell.execute_reply.started":"2023-05-02T21:46:01.107422Z","shell.execute_reply":"2023-05-02T21:46:01.114394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second path of the UNET architecture upsamples input using Conv2DTranspose, which in effect does the opposite of a Conv2D layer.","metadata":{}},{"cell_type":"code","source":"def upsample(num_filters, kernel_size, apply_dropout=False):\n    normal_init = random_normal_initializer(0, 0.02)\n    gamma_init = RandomNormal(mean=0, stddev=0.02)\n\n    model = Sequential()\n    model.add(Conv2DTranspose(num_filters, kernel_size, strides=2, padding='same', kernel_initializer=normal_init, use_bias=False))\n    model.add(InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        model.add(Dropout(0.5))\n\n    model.add(ReLU())\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:01.117104Z","iopub.execute_input":"2023-05-02T21:46:01.117803Z","iopub.status.idle":"2023-05-02T21:46:01.134310Z","shell.execute_reply.started":"2023-05-02T21:46:01.117766Z","shell.execute_reply":"2023-05-02T21:46:01.133473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Generator\nThe two paths of downsampling and upsampling are combined to build the generator.  As a UNET architecture, the generator first downsamples the image and then upsamples the result of the downsamples.  Because of how the downsampler and upsampler connect, the generator can remember long-range dependencies through the image processing layers.","metadata":{}},{"cell_type":"code","source":"def Generator():\n    inputs = Input(shape=[256, 256, 3])\n    \n    u_down = [\n        downsample(64, 4, apply_instancenorm=False), # After this, image size is 128\n        downsample(128, 4), # After this, image size is 64\n        downsample(256, 4), # After this, image size is 32\n        downsample(512, 4), # After this, image size is 16\n        downsample(512, 4), # After this, image size is 8\n        downsample(512, 4), # After this, image size is 4\n        downsample(512, 4), # After this, image size is 2\n        downsample(512, 4), # After this, image size is 1\n    ]\n    \n    u_up = [\n        upsample(512, 4, apply_dropout=True), # After this, image size is 2\n        upsample(512, 4, apply_dropout=True), # After this, image size is 4\n        upsample(512, 4, apply_dropout=True), # After this, image size is 8\n        upsample(512, 4), # After this, image size is 16\n        upsample(256, 4), # After this, image size is 32\n        upsample(128, 4), # After this, image size is 64\n        upsample(64, 4), # After this, image size is 128\n    ]\n    \n    normal_init = random_normal_initializer(0, 0.02)\n    final = Conv2DTranspose(3, 4, strides=2, padding=\"same\", kernel_initializer=normal_init, activation='tanh') #  After this, image size is 128\n\n    # Build UNET Neural Network layers\n    x = inputs\n    \n    # Downsample\n    skips = []\n    for down in u_down:\n        x = down(x)\n        skips.append(x)\n    skips = reversed(skips[:-1])\n    \n    # Upsample and keep the skip connections\n    for up, skip in zip(u_up, skips):\n        x = up(x)\n        x = Concatenate()([x, skip])\n        \n    x = final(x)\n    \n    return Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:01.135770Z","iopub.execute_input":"2023-05-02T21:46:01.136446Z","iopub.status.idle":"2023-05-02T21:46:01.147599Z","shell.execute_reply.started":"2023-05-02T21:46:01.136406Z","shell.execute_reply":"2023-05-02T21:46:01.146825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Discriminator\nThe discriminator classifies the input as fake (generated) or real.  The output of this discriminator is a small 2D image with high pixel values indicating real and low pixel values indicating fake classification.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    normal_init = random_normal_initializer(0, 0.02)\n    gamma_init = RandomNormal(mean=0, stddev=0.02)\n    \n    inputs = Input(shape=[256, 256, 3], name=\"input_image\")\n    \n    x = inputs\n    \n    x = downsample(64, 4, apply_instancenorm=False)(x) # After this, image size is 128\n    x = downsample(128, 4)(x) # After this, image size is 64\n    x = downsample(256, 4)(x) # After this, image size is 32\n    \n    x = ZeroPadding2D()(x) # After this, image size is 34\n    x = Conv2D(512, 4, strides=1, kernel_initializer=normal_init, use_bias=False)(x) # After this, image size is 31\n    x = InstanceNormalization(gamma_initializer=gamma_init)(x) # After this, image size is 31\n    x = LeakyReLU()(x) # After this, image size is 31\n    x = ZeroPadding2D()(x) # After this, image size is 33\n    x = Conv2D(1, 4, strides=1, kernel_initializer=normal_init)(x) # After this, image size is 30\n    \n    return Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:01.149004Z","iopub.execute_input":"2023-05-02T21:46:01.149657Z","iopub.status.idle":"2023-05-02T21:46:01.163847Z","shell.execute_reply.started":"2023-05-02T21:46:01.149622Z","shell.execute_reply":"2023-05-02T21:46:01.163151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collect the Generators and Discriminators\nWithin the scope of a strategy, the Generators and Discriminators for the Monet art and for the photographs are defined.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_gen = Generator()\n    monet_dsc = Discriminator()\n    \n    photo_gen = Generator()\n    photo_dsc = Discriminator()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:01.168306Z","iopub.execute_input":"2023-05-02T21:46:01.169228Z","iopub.status.idle":"2023-05-02T21:46:04.145307Z","shell.execute_reply.started":"2023-05-02T21:46:01.169163Z","shell.execute_reply":"2023-05-02T21:46:04.144277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the generator not trained yet, the generated Monet does not yet look correct.","metadata":{}},{"cell_type":"code","source":"to_monet = monet_gen(example_photo)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original\")\nplt.imshow(example_photo[0]*0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Generated Monet\")\nplt.imshow(to_monet[0]*0.5 + 0.5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:04.147860Z","iopub.execute_input":"2023-05-02T21:46:04.148342Z","iopub.status.idle":"2023-05-02T21:46:12.443502Z","shell.execute_reply.started":"2023-05-02T21:46:04.148313Z","shell.execute_reply":"2023-05-02T21:46:12.442515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Model\nAn implementation of a Model is defined for this GAN that will transform a photo into a Monet twice.  This twice transformed Monet will then be used to calculate similarity.","metadata":{}},{"cell_type":"code","source":"class CycleGAN(Model):\n    def __init__(self, monet_gen, monet_dsc, photo_gen, photo_dsc, cycle=10):\n        super(CycleGAN, self).__init__()\n        \n        self.monet_gen = monet_gen\n        self.monet_dsc = monet_dsc\n        \n        self.photo_gen = photo_gen\n        self.photo_dsc = photo_dsc\n        \n        self.cycle = cycle\n        \n    def compile(self, monet_gen_optimizer, monet_dsc_optimizer, photo_gen_optimzier, photo_dsc_optimizer, gen_loss_fn, dsc_loss_fn, cyc_loss_fn, idy_loss_fn):\n        super(CycleGAN, self).compile()\n\n        self.monet_gen_optimizer = monet_gen_optimizer\n        self.monet_dsc_optimizer = monet_dsc_optimizer\n        \n        self.photo_gen_optimizer = photo_gen_optimizer\n        self.photo_dsc_optimizer = photo_dsc_optimizer\n        \n        self.gen_loss_fn = gen_loss_fn\n        self.dsc_loss_fn = dsc_loss_fn\n        self.cyc_loss_fn = cyc_loss_fn\n        self.idy_loss_fn = idy_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with GradientTape(persistent=True) as tape:\n            \n            # Photo --> Monet --> Photo\n            faked_monet = self.monet_gen(real_photo, training=True)\n            cycld_photo = self.photo_gen(faked_monet, training=True)\n            \n            # Monet --> Photo --> Monet\n            faked_photo = self.photo_gen(real_monet, training=True)\n            cycld_monet = self.monet_gen(faked_photo, training=True)\n            \n            # Generating itself\n            same_monet = self.monet_gen(real_monet, training=True)\n            same_photo = self.photo_gen(real_photo, training=True)\n            \n            # Discriminator used to check real images\n            dsc_real_monet = self.monet_dsc(real_monet, training=True)\n            dsc_real_photo = self.photo_dsc(real_photo, training=True)\n            \n            # Discriminator used to check fake images\n            dsc_faked_monet = self.monet_dsc(faked_monet, training=True)\n            dsc_faked_photo = self.photo_dsc(faked_photo, training=True)\n            \n            # Calculate generator loss\n            monet_gen_loss = self.gen_loss_fn(dsc_faked_monet)\n            photo_gen_loss = self.gen_loss_fn(dsc_faked_photo)\n            \n            # Calculate total cycle consistency loss\n            tot_cycle_loss = self.cyc_loss_fn(real_monet, cycld_monet, self.cycle) + self.cyc_loss_fn(real_photo, cycld_photo, self.cycle)\n            \n            # Calculate total generator loss\n            tot_monet_gen_loss = monet_gen_loss + tot_cycle_loss + self.idy_loss_fn(real_monet, same_monet, self.cycle)\n            tot_photo_gen_loss = photo_gen_loss + tot_cycle_loss + self.idy_loss_fn(real_photo, same_photo, self.cycle)\n            \n            # Calculate discriminator loss\n            monet_dsc_loss = self.dsc_loss_fn(dsc_real_monet, dsc_faked_monet)\n            photo_dsc_loss = self.dsc_loss_fn(dsc_real_photo, dsc_faked_photo)\n            \n        # Calculate the gradients for the generator\n        monet_gen_grads = tape.gradient(tot_monet_gen_loss, self.monet_gen.trainable_variables)\n        photo_gen_grads = tape.gradient(tot_photo_gen_loss, self.photo_gen.trainable_variables)\n        \n        # Apply the gradients to the generator optimizer\n        self.monet_gen_optimizer.apply_gradients(zip(monet_gen_grads, self.monet_gen.trainable_variables))\n        self.photo_gen_optimizer.apply_gradients(zip(photo_gen_grads, self.photo_gen.trainable_variables))\n        \n        # Calculate the gradients for the discriminator\n        monet_dsc_grads = tape.gradient(monet_dsc_loss, self.monet_dsc.trainable_variables)\n        photo_dsc_grads = tape.gradient(photo_dsc_loss, self.photo_dsc.trainable_variables)\n        \n        # Apply the gradients to the discriminator optimizer\n        self.monet_dsc_optimizer.apply_gradients(zip(monet_dsc_grads, self.monet_dsc.trainable_variables))\n        self.photo_dsc_optimizer.apply_gradients(zip(photo_dsc_grads, self.photo_dsc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": tot_monet_gen_loss,\n            \"photo_gen_loss\": tot_photo_gen_loss,\n            \"monet_dsc_loss\": monet_dsc_loss,\n            \"photo_dsc_loss\": photo_dsc_loss\n        }       ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.445102Z","iopub.execute_input":"2023-05-02T21:46:12.445768Z","iopub.status.idle":"2023-05-02T21:46:12.463011Z","shell.execute_reply.started":"2023-05-02T21:46:12.445731Z","shell.execute_reply":"2023-05-02T21:46:12.461945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Loss Functions\nThe loss functions will calculate how accurately the generator and discriminator work.\n\n### Discriminator Loss\nThe loss functions compare real images to a matrix of 1s and fake images to a matrix of 0s.  The perfect discriminator will output all 1s for real images and all 0s for afake images.  \n\nWith this approach in mind, the discriminator loss outputs the average of the real image loss and the generated image loss. ","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real_img, gen_img):\n        real_loss = BinaryCrossentropy(from_logits=True, reduction=Reduction.NONE)(ones_like(real_img), real_img)\n        gen_loss = BinaryCrossentropy(from_logits=True, reduction=Reduction.NONE)(zeros_like(gen_img), gen_img)\n        \n        tot_dsc_loss = real_loss + gen_loss\n        \n        return tot_dsc_loss * 0.5        ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.464659Z","iopub.execute_input":"2023-05-02T21:46:12.465165Z","iopub.status.idle":"2023-05-02T21:46:12.480204Z","shell.execute_reply.started":"2023-05-02T21:46:12.465122Z","shell.execute_reply":"2023-05-02T21:46:12.479198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generator Loss\nThe loss function for the generator relies on the fact that the discriminator output will be only 1s.  So the loss function will compare the generated image to a matrix of ones.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def generator_loss(gen_img):\n        return BinaryCrossentropy(from_logits=True, reduction=Reduction.NONE)(ones_like(gen_img), gen_img)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.482006Z","iopub.execute_input":"2023-05-02T21:46:12.482395Z","iopub.status.idle":"2023-05-02T21:46:12.491167Z","shell.execute_reply.started":"2023-05-02T21:46:12.482343Z","shell.execute_reply":"2023-05-02T21:46:12.490251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cycle Consistency Loss\nWhen the original photo is transformed to a Monet art and back, the resulting image should be similar to the original image.  The loss function can so be implemented to return a multiple or fraction of the average of the difference between the original and resulting images.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def cycle_loss(real_img, cycld_img, LAMBDA):\n        return LAMBDA * reduce_mean(abs(real_img - cycld_img))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.492495Z","iopub.execute_input":"2023-05-02T21:46:12.493410Z","iopub.status.idle":"2023-05-02T21:46:12.506194Z","shell.execute_reply.started":"2023-05-02T21:46:12.493371Z","shell.execute_reply":"2023-05-02T21:46:12.505153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Identity Loss\nFinally the identity loss function compares the image with it's own generator (i.e. photo with the photo generator and monet with monet generator).  The generator for each should ideally generate the same image as the original.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_img, same_img, LAMBDA):\n        return LAMBDA * 0.5 * reduce_mean(abs(real_img - same_img))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.507588Z","iopub.execute_input":"2023-05-02T21:46:12.508010Z","iopub.status.idle":"2023-05-02T21:46:12.518092Z","shell.execute_reply.started":"2023-05-02T21:46:12.507960Z","shell.execute_reply":"2023-05-02T21:46:12.517051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model\nWith the above implementations for generator, discriminator, and loss functions, the CycleGAN model can be trained.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_gen_optimizer = Adam(2e-4, beta_1=0.5)\n    monet_dsc_optimizer = Adam(2e-4, beta_1=0.5)\n    \n    photo_gen_optimizer = Adam(2e-4, beta_1=0.5)\n    photo_dsc_optimizer = Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.519633Z","iopub.execute_input":"2023-05-02T21:46:12.520025Z","iopub.status.idle":"2023-05-02T21:46:12.530536Z","shell.execute_reply.started":"2023-05-02T21:46:12.519980Z","shell.execute_reply":"2023-05-02T21:46:12.529566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = CycleGAN(monet_gen, monet_dsc, photo_gen, photo_dsc)\n    \n    model.compile(\n        monet_gen_optimizer = monet_gen_optimizer, \n        monet_dsc_optimizer = monet_dsc_optimizer, \n        photo_gen_optimzier = photo_gen_optimizer, \n        photo_dsc_optimizer = photo_dsc_optimizer, \n        gen_loss_fn = generator_loss, \n        dsc_loss_fn = discriminator_loss, \n        cyc_loss_fn = cycle_loss, \n        idy_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.531971Z","iopub.execute_input":"2023-05-02T21:46:12.532636Z","iopub.status.idle":"2023-05-02T21:46:12.560465Z","shell.execute_reply.started":"2023-05-02T21:46:12.532594Z","shell.execute_reply":"2023-05-02T21:46:12.559537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(Dataset.zip((monet_ds, photo_ds)), epochs=12, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:46:12.561705Z","iopub.execute_input":"2023-05-02T21:46:12.562149Z","iopub.status.idle":"2023-05-02T21:51:38.375318Z","shell.execute_reply.started":"2023-05-02T21:46:12.562111Z","shell.execute_reply":"2023-05-02T21:51:38.374302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results and Analysis\nWith the CycleGAN built, compiled, and fit, images may now be translated via the prepared model.  Here follow some example translations of a source photo to a monet style.","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(5, 4, figsize=(12, 12))\n\ncol = 0\n\n# Arrange comparisons in a 4 column format\nfor i, img in enumerate(photo_ds.take(10)):\n    pred = monet_gen(img, training=False)[0].numpy()\n    pred = (pred * 127.5 + 127.5).astype(np.uint8)  # Scale the pixels up to rgb values\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8) # Scale the pixels up to rgb values\n    \n    ax[i//2, 2*col + 0].imshow(img)\n    ax[i//2, 2*col + 0].set_title(\"Original\")\n    ax[i//2, 2*col + 0].axis(\"off\")\n    \n    ax[i//2, 2*col + 1].imshow(pred)\n    ax[i//2, 2*col + 1].set_title(\"Monet\")\n    ax[i//2, 2*col + 1].axis(\"off\")\n    \n    col += 1\n    if col == 2:\n        col = 0\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:52:19.658017Z","iopub.execute_input":"2023-05-02T21:52:19.658790Z","iopub.status.idle":"2023-05-02T21:52:21.688528Z","shell.execute_reply.started":"2023-05-02T21:52:19.658748Z","shell.execute_reply":"2023-05-02T21:52:21.687371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Monet translations seem to match the original photos well, and yet also have applied a generative brush toward the Monet art style.  \n\n## Create the submission file\nWith the model ready, the submission file containing 7K to 10K images translated to the Monet style may be preprared.","metadata":{}},{"cell_type":"code","source":"! mkdir \"/kaggle/working/generated_images\"","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:53:08.209183Z","iopub.execute_input":"2023-05-02T21:53:08.210187Z","iopub.status.idle":"2023-05-02T21:53:09.305076Z","shell.execute_reply.started":"2023-05-02T21:53:08.210129Z","shell.execute_reply":"2023-05-02T21:53:09.303440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    pred = monet_gen(img, training=False)[0].numpy()\n    pred = (pred * 127.5 + 127.5).astype(np.uint8) # Scale pixels up to the rgb values\n    im = PIL.Image.fromarray(pred)\n    im.save(f\"{WORKING_DIR}/generated_images/gen_monet_{i}.jpg\")\n    if i%100 == 0:\n        print(f\"{i} images saved\")\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:53:39.124170Z","iopub.execute_input":"2023-05-02T21:53:39.124859Z","iopub.status.idle":"2023-05-02T21:53:43.073782Z","shell.execute_reply.started":"2023-05-02T21:53:39.124821Z","shell.execute_reply":"2023-05-02T21:53:43.072754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive(f\"{WORKING_DIR}/images\", \"zip\", f\"{WORKING_DIR}/generated_images\")","metadata":{"execution":{"iopub.status.busy":"2023-05-02T21:53:49.201136Z","iopub.execute_input":"2023-05-02T21:53:49.201552Z","iopub.status.idle":"2023-05-02T21:53:49.233295Z","shell.execute_reply.started":"2023-05-02T21:53:49.201513Z","shell.execute_reply":"2023-05-02T21:53:49.232171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThe CycleGAN proved to be an effective model for translating photos to an art style.\n\nThis journal built and trained a CycleGAN model on several hundred Monet images and almost 1000 photo images. The goal of the model was to translate photos into the Monet art style. CycleGAN was selected as a good choice for image translations from one domain to another.\n\nAlong the way, several loss functions were defined to evaluate and fit the generator and discriminator. In the end, the model was fit over 25 epochs and the translations generated by the model looked quite convincing as part of the Monet art style.\n\nOverall, we were successful in building and training a CycleGAN model that can translate photos into the Monet art style. The translations generated by the model are of high quality and are visually appealing. This model has the potential to be used for a variety of applications, such as creating art, generating images for marketing purposes, or even creating new art styles.\n\nThank you to Amy Jang, who offered a [tutorial](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook) that guided this author's understanding.","metadata":{}}]}